% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}

\newtheorem{case}{Case}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\mle}{\textnormal{mle}}
\newcommand{\tr}{\textnormal{tr}}
\newcommand{\iCup}{$\small$\bigcup\limits_{i \in I}$\normalsize$}
\newcommand{\iCap}{\bigcap\limits_{i \in I}}
\newcommand{\diam}{\textnormal{diam}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\dist}{\textnormal{dist}}


 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
\title{Homework 1}
\author{George Duncan CX 4240} 
\maketitle
\date


\begin{problem}\label{1)}
	$\Sigma = $
	$\begin{bmatrix}
	1&r\\r&1
	\end{bmatrix}$
	\\
	We must have $\Sigma$ positive definite. By the Cauchy-Schwartz inequality:\tabularnewline\\
	$\textnormal{cov}(X_1, X_2)^2 \leq \textnormal{cov}(X_1, X_1) \textnormal{cov}(X_2, X_2)$\\
	$\textnormal{cov}(X_1, X_2)^2 \leq \textnormal{var}(X_1) \textnormal{var}(X_2)$\\
	$r^2 \leq 1$ ... $r\in[-1, 1]$\\
	For both $r = -1, r = 1$, the determinant of the matrix is 0, therefore the matrix is degenerate. Thus: $r \in (-1, 1)$. We can use the following expansion for the joint probability density function:\\
	$$\Pr(X = x) = p(X) = \frac{1}{2\pi}\det(\Sigma)^{-1}\exp(-q(X-\mu))$$, where $q$ is defined by the following:\\
	$$q(Z) = \frac{\frac{z_1^2}{\Sigma_{1,1}^2} - \frac{2\sqrt{1-\det(\Sigma)}z_1z_2}{\tr(\Sigma)} + \frac{z_2^2}{\Sigma_{2,2}^2}}{2\det(\Sigma)}$$
	Substituting in the values form this problem:\\
	$$p(X) = \frac{1}{2\pi(1-r^2)}\exp(-\frac{(x_1-\mu_1)^2 - 2r(x_1-\mu_1)(x_2-\mu_2) + (x_2-\mu_2)^2}{2(1-r^2)})$$
	\\
	\\
	\\
	For the eigen values we must set the determinant of the matrix equal to 0:\\
	$\det(\Sigma) = 0 = (1-\lambda)^2-r^2$, thus $\lambda = 1 \pm r$. Consider the positive case $\lambda_1$, and the negative case $\lambda_2$. For each $\lambda$ we can find the corresponding eigen vectors accordingly:\\
	$(\Sigma - \lambda I)v = 0$\\
	$\begin{bmatrix}
	-r&r\\r&-r\\
	\end{bmatrix}v_1 = $
	$\begin{bmatrix}0\\0 \end{bmatrix}$, thus $v_1 = c(1, 1) : c\in \R$, after normalization:
	$v_1 = (\frac{\sqrt(2)}{2}, \frac{\sqrt(2)}{2})$ 
	$\begin{bmatrix}
	r&r\\r&r\\
	\end{bmatrix}v_1 = $
	$\begin{bmatrix}0\\0 \end{bmatrix}$, thus $v_2 = c(1, -1) : c\in \R$, after normalization:
	$v_2 = (\frac{\sqrt(2)}{2}, \frac{-\sqrt(2)}{2})$ 
\end{problem}
\pagebreak
\begin{problem}\label{2)}
Suppose $X$ is a random variable with the following density:\\
$$f_X(x)  = \begin{cases} 0.5 & x = -1, 1\\ 0 & x\neq -1,1\end{cases}$$
The expected value of $X$ is given by $\mathbb{E}(X) = \sum\limits_{x_i \in \Omega(X)}x_ip(x_i) = 0.5(-1) + 0.5(1) = 0$\\
Suppose $Y$ is a random variable with the standard normal distribution, with density $f_Y(y)$.\\
Let $Z = XY$, we have $Z$ is dependent on $Y$ as the value of $Z$ is restricted to $\pm Y$, from its original domain of $\R$\\
$p_Z(z) = \sum\limits_{x_i \in \Omega(X)}p_X(x_i)p_Y(\frac{z}{x_i}) = \frac{1}{2}p_Y(z) + \frac{1}{2}p_Y(-z)$. The standard normal distribution is symmetrical therefore:
$ = \frac{1}{2}p_Y(z) + \frac{1}{2}p_Y(z) = p_Y(z) = p_Z(z)$, the two variables have identical densities, therefore $Z$ is a standard normal distribution: $\mathcal{N}(0, 1)$\\
\begin{align}
\textnormal{cov}(Y,Z) &= \mathbb{E}(YZ) - \mathbb{E}(Y)\mathbb{E}(Z) \\
&= \mathbb{E}(YZ) - (0)(0) \\
&= \mathbb{E}(YZ) \\
&= \mathbb{E}(XY^2) \\
&= \sum\limits_{x_i \in \Omega(X)}\int\limits_{y \in \Omega(X)}p_{X,Y}(x_i,y)xy^2dy \\
&=  \sum\limits_{x_i \in \Omega(X)}\int\limits_{y \in \Omega(X)}p_X(x_i)p_Y(y)xy^2dy \\
&= \frac{-1}{2}\int\limits_{y \in \Omega(X)}p_Y(y)y^2dy + \frac{1}{2}\int\limits_{y \in \Omega(X)}p_Y(y)y^2dy \\
\textnormal{cov}(Y,Z) &= 0
\end{align}
Y and Z are uncorrelated.\\
\end{problem}
\pagebreak
\begin{problem}\label{3.1)}
$p_1 = \begin{cases} 2\theta & x = 1 \\ 1-2\theta & x = 0 \end{cases}$ $p_2 = \begin{cases} \theta & x = 1 \\ 1-\theta & x = 0 \end{cases}$\\
$p_{t1} = p_1(0) = 1-2\theta$ and $p_{t2} = p_2(0) = 1-\theta$
We have for probability $p \geq 0$, thus $\theta \in [0, \frac{1}{2}]$
$l = p_1(1)p_2(1)p_1(0)p_2(0)p_1(0)p_2(1) = 4\theta^3(1-4\theta+5\theta^2-2\theta^3)$\\

\begin{align}
\mle(\theta) &= \theta : l(\theta) = \max(l(\theta) : \theta \in \{\theta : \frac{dl}{d\theta} = 0, 0, \frac{1}{2}\}\\
\frac{dl}{d\theta} &= (12\theta^2 - 64\theta^3+100\theta^4-48\theta^5) = -4\theta^2(\theta-1)(3\theta-1)(4\theta-3)\\
\mle(\theta) &= \theta : l(\theta) = \max(l(\theta) : \theta \in \{0, \frac{1}{3}, \frac{1}{2}, 1\}\textnormal{, restricting to the domain of }\theta:\\
&= \theta : l(\theta) = \max(l(\theta) : \theta \in \{0, \frac{1}{3}, \frac{1}{2}\}\\l(0) &= 0, l(\frac{1}{3}) = \frac{16}{3^6}, l(\frac{1}{2}) = 0\\
\mle(\theta) &= \frac{1}{3}
\end{align}

\end{problem}

\begin{problem}\label{3.2)}
Suppose $\theta < \max(x_1, ..., x_n)$ then $\exists i \in \{1, 2, ..., n\}\ :\ x_i > \theta$, therefore $f(x_i) = 0$, therefore $l(\theta) = 0$.\\
Suppose $\theta > \max(x_1, ..., x_n)$ and $\theta' = \max(x_1, ..., x_n)$. $\forall i \in \{1, 2, ..., n\}\ f(x_i)=\frac{1}{\theta}$ thus $l(\theta) = \frac{1}{\theta^n}$ and $l(\theta') = \frac{1}{(\theta')^n}$, because $l'(\theta) < 0 \forall \theta:$ we have $\theta' < \theta \implies l(\theta') > l(\theta)$, thus $\max(l(\theta)) = l(\max(x_1, ..., x_n))$, therefore $\mle(\theta) = \max(x_1, ..., x_n)$
\end{problem}
\pagebreak
\begin{problem}\label{4)}
(Note: logs are all in base 2)
$p_X(x) = \sum\limits_{y_i \in \Omega(Y)}p_{X,Y}(x,y_i) = \begin{cases} \frac{1}{2} & x = 0\\ \frac{1}{2} & x = 1 \end{cases}$\\
$H(X) = \sum\limits_{x_i \in \Omega(X)}-\log(p_X(x_i))$\\
$H(X) = -\log(\frac{1}{2}) = 1$\\
\\
$H(X\vert Y) = \sum\limits_{(x_i,y_i) \in (\Omega(X), \Omega(Y))}p_{X, Y}(x_i, y_i)(-\log(p_{X\vert Y}(x_i \vert y_i)))$\\
$H(X\vert Y) = \frac{1}{4}(-\log(\frac{1}{3})) + \frac{1}{2}(-\log(\frac{2}{3})) + \frac{1}{4}(-\log(1))$\\
$\hspace*{325pt}...H(X\vert Y) = \frac{3\log(3)-2}{4}$\\
$H(Y\vert X) = \sum\limits_{(x_i,y_i) \in (\Omega(X), \Omega(Y))}p_{X, Y}(x_i, y_i)(-\log(p_{Y\vert X}(y_i \vert x_i)))$\\
$H(Y\vert X) = \frac{1}{4}(-\log(\frac{1}{2})) + \frac{1}{4}(-\log(\frac{1}{2})) + \frac{1}{2}(-\log(1))$\\
$\hspace*{325pt}...H(Y\vert X) = \frac{1}{2}$\\
$I(X, Y) = \sum\limits_{(x_i,y_i) \in (\Omega(X), \Omega(Y))}p_{X, Y}(x_i, y_i)\log(\frac{p_{X,Y}(x_i, y_i))}{p_X(x_i)p_Y(y_i)}$\\
$I(X, Y) = \sum\limits_{(x_i,y_i) \in (\Omega(X), \Omega(Y))}p_{X, Y}(x_i, y_i)\log(\frac{p_{X,Y}(x_i, y_i))}{p_X(x_i)p_Y(y_i)}$\\
$I(X, Y) = \frac{1}{4}\log(\frac{2}{3}) + \frac{1}{4}\log(2) + \frac{1}{2}\log(\frac{4}{3})$\\
$\hspace*{325pt}...I(X,Y) = \frac{3}{2} - \frac{3}{4}\log(3)$\\
$H(X, Y) = H(X) + H(Y\vert X) = 1 + \frac{1}{2} = \hspace*{118pt}... H(X, Y) = \frac{3}{2}$
\\
\begin{theorem}.
Show $X, Y$ are independent $\implies$ $H(X\vert Y) = H(X)$
\begin{proof}
Suppose $X, Y$ are independent, then:\\
$H(X\vert Y) = \sum\limits_{(x_i,y_i) \in (\Omega(X), \Omega(Y))}p_{X, Y}(x_i, y_i)(-\log(p_{X\vert Y}(x_i \vert y_i)))$\\
By independence we have $p_{X\vert Y} = p_X$, and $p_{X,Y} = p_Xp_Y$, thus:\\
$H(X\vert Y) = \sum\limits_{(x_i,y_i) \in (\Omega(X), \Omega(Y))}p_{X, Y}(x_i, y_i)(-\log(p_X(x_i)))$\\
$H(X\vert Y) = \mathbb{E}(-\log(p_X)) = H(X)$
\end{proof}
The same holds for $H(Y\vert X)$
\end{theorem}


\begin{theorem}.
Show $X, Y$ are independent $\implies$ $H(X, Y) = H(X) + H(Y)$
\begin{proof}
Suppose $X, Y$ are independent, then:\\
$H(X, Y) = H(X) + H(Y\vert X) = H(X) + H(Y)$, as follows from the above proof
\end{proof}
\end{theorem}


\begin{theorem}.
Show $I(X; X) = H(X)$:
\begin{proof}
$I(X; X) = \sum\limits_{x_i \in \Omega(X)}p_X(x_i)\log(\frac{p_X(x_i)}{p_X(x_i)p_X(x_i)})$\\
$I(X; X) = \sum\limits_{x_i \in \Omega(X)}p_X(x_i)\log(\frac{1}{p_X(x_i)})$\\
$I(X; X) = \sum\limits_{x_i \in \Omega(X)}p_X(x_i)(-\log(p_X(x_i)))$\\
$I(X; X) = H(X)$\\
\end{proof}
\end{theorem}
\end{problem}
\end{document}